# Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead

**Authors:** C. Rudin

**Year:** 2018 | **Venue:** Nature Machine Intelligence

**Citations:** 7,038 (Influential: 389)


## TL;DR (AI Summary)
This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications whereinterpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.


**DOI:** 10.1038/s42256-019-0048-x
